{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA4: Relation classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from gensim.utils import tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, fbeta_score, make_scorer, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PairExample = namedtuple('PairExample',\n",
    "    'entity_1, entity_2, snippet')\n",
    "Snippet = namedtuple('Snippet',\n",
    "    'left, mention_1, middle, mention_2, right, direction')\n",
    "def load_data(file, verbose=True):\n",
    "    f = open(file,'r', encoding='utf-8')\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i,line in enumerate(f):\n",
    "        instance = json.loads(line)\n",
    "\n",
    "        instance_tuple = PairExample(instance['entity_1'],instance['entity_2'],[])\n",
    "        \n",
    "        for snippet in instance['snippet']:\n",
    "            try:\n",
    "                snippet_tuple = Snippet(snippet['left'],snippet['mention_1'],snippet['middle'],\n",
    "                                   snippet['mention_2'],snippet['right'],\n",
    "                                    snippet['direction'])\n",
    "                instance_tuple.snippet.append(snippet_tuple)\n",
    "                \n",
    "                data.append(instance_tuple)\n",
    "                labels.append(instance['relation'])\n",
    "                instance_tuple = PairExample(instance['entity_1'],instance['entity_2'],[])\n",
    "                \n",
    "            except:\n",
    "                print(instance)\n",
    "\n",
    "    return data,labels\n",
    "    \n",
    "train_data, train_labels = load_data('data/train.json.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set statistics:\n",
      "                                rel_examples\n",
      "relation               examples /all_examples\n",
      "--------               --------    -------\n",
      "author                    13113       0.31\n",
      "worked_at                  3669       0.09\n",
      "has_spouse                13061       0.31\n",
      "capital                    9427       0.22\n",
      "NO_REL                     3068       0.07\n",
      "--------               --------    -------\n",
      "Total                     42338       1.00\n"
     ]
    }
   ],
   "source": [
    "# Statistics over relations\n",
    "def print_stats(labels):\n",
    "    labels_counts = Counter(labels)\n",
    "    print('{:20s} {:>10s} {:>10s}'.format('', '', 'rel_examples'))\n",
    "    print('{:20s} {:>10s} {:>10s}'.format('relation', 'examples', '/all_examples'))\n",
    "    print('{:20s} {:>10s} {:>10s}'.format('--------', '--------', '-------'))\n",
    "\n",
    "    for k,v in labels_counts.items():\n",
    "        print('{:20s} {:10d} {:10.2f}'.format(k, v, v /len(labels)))\n",
    "    print('{:20s} {:>10s} {:>10s}'.format('--------', '--------', '-------'))\n",
    "    print('{:20s} {:10d} {:10.2f}'.format('Total', len(labels), len(labels) /len(labels)))\n",
    "\n",
    "print('Train set statistics:')\n",
    "print_stats(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get full context\n",
    "\"\"\"def get_context(data, embed_mode=False):\n",
    "    all_data = []\n",
    "    for instance in data:\n",
    "        #s_context = []\n",
    "        for s in instance.snippet:\n",
    "            if embed_mode:\n",
    "                all_data.append(' '.join((s.left, s.mention_1.replace(\" \", \"_\"), s.middle, s.mention_2.replace(\" \", \"_\"), s.right)))\n",
    "            else:\n",
    "                all_data.append(' '.join((s.left, \"entity1\", s.middle, \"entity2\", s.right)))\n",
    "                # s_context.append(' '.join((s.left, s.mention_1, s.middle, s.mention_2, s.right)))\n",
    "                # s_context.append(' '.join((s.left, s.middle, s.right)))\n",
    "        #all_data.append(' '.join(s_context))\n",
    "\n",
    "    print(len(all_data))\n",
    "    return all_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. EXTRACT FEATURES and BUILD CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SelectContext(data, verbose=True):\n",
    "    \"\"\"BOW feature extraction\"\"\"\n",
    "    only_context_data = []\n",
    "    for instance in data:\n",
    "        \n",
    "        instance_context = []\n",
    "        for s in instance.snippet:\n",
    "            context = s.left + \" m_1 \" + s.middle + \" m_2 \" + s.right\n",
    "            instance_context.append(context)\n",
    "        only_context_data.append(' '.join(instance_context))\n",
    "    if verbose:\n",
    "        print(len(only_context_data))\n",
    "        print(only_context_data[0])\n",
    "    return only_context_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "thirty and his life and career were riding high . In 1941 , shortly after the death of his father , Mercer began an intense affair with nineteen-year-old m_1 while she was engaged to composer m_2 . Garland married Rose to temporarily stop the affair , but the effect on Mercer lingered , adding to the emotional depth of his lyrics . Their affair\n"
     ]
    }
   ],
   "source": [
    "test_feat = SelectContext(train_data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ExractSimpleFeatures(data, verbose=True):\n",
    "    featurized_data = []\n",
    "    for instance in data:\n",
    "        featurized_instance = {'mid_words': '', 'distance': np.inf}\n",
    "        for s in instance.snippet:\n",
    "            if len(s.middle.split()) < featurized_instance['distance']:\n",
    "                featurized_instance['mid_words'] = s.middle\n",
    "                featurized_instance['distance'] = len(s.middle.split())\n",
    "        featurized_data.append(featurized_instance)\n",
    "    if verbose:\n",
    "        print(len(featurized_data))\n",
    "        print(featurized_data[0])\n",
    "        print(featurized_data[1])\n",
    "    return featurized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'distance': 6, 'mid_words': 'while she was engaged to composer'}\n",
      "{'distance': 1, 'mid_words': 'by'}\n"
     ]
    }
   ],
   "source": [
    "test_feat = ExractSimpleFeatures(train_data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LengthOfEntities(data, verbose=True):\n",
    "    featurized_data = []\n",
    "    for instance in data:\n",
    "        featurized_instance = {\n",
    "            'entity1_len': len(instance.entity_1.split(\"_\")),\n",
    "            'entity2_len': len(instance.entity_2.split(\"_\")),\n",
    "            'combined_len': len(instance.entity_1.split(\"_\")) + len(instance.entity_2.split(\"_\"))\n",
    "        }\n",
    "        featurized_data.append(featurized_instance)\n",
    "    if verbose:\n",
    "        print(len(featurized_data))\n",
    "        print(featurized_data[0])\n",
    "        print(featurized_data[1])\n",
    "    return featurized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'combined_len': 4, 'entity1_len': 2, 'entity2_len': 2}\n",
      "{'combined_len': 7, 'entity1_len': 5, 'entity2_len': 2}\n"
     ]
    }
   ],
   "source": [
    "test_feat = LengthOfEntities(train_data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleFeaturizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, featurizer):\n",
    "        self.featurizers = featurizer\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return ExractSimpleFeatures(X, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EntityLengthFeaturizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each isntance for DictVectorizer\"\"\"\n",
    "    def __init__(self, featurizer):\n",
    "        self.featurizers = featurizer\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return LengthOfEntities(X, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BowFeaturizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"BOW featurizer\"\"\"\n",
    "    def __init__(self, featurizer):\n",
    "        self.featurizers = featurizer\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return SelectContext(X, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Transform labels to nimeric values\n",
    "le = LabelEncoder()\n",
    "train_labels_featurized = le.fit_transform(train_labels)\n",
    "\n",
    "length_pipe = make_pipeline(EntityLengthFeaturizer(LengthOfEntities), DictVectorizer())\n",
    "\n",
    "bow_pipe = make_pipeline(BowFeaturizer(SelectContext), CountVectorizer(ngram_range=(1,3)))\n",
    "# bow_pipe = make_pipeline(BowFeaturizer(SelectContext), CountVectorizer())\n",
    "\n",
    "simple_pipe = make_pipeline(SimpleFeaturizer(ExractSimpleFeatures), DictVectorizer())\n",
    "\n",
    "#syntax_pipe = make_pipeline(DependencyPath(FindDepPath), DictVectorizer())\n",
    "\n",
    "#clf = make_pipeline(FeatureUnion(transformer_list=[\n",
    "    #('length_pipeline', length_pipe),\n",
    "    #('bow_pipeline', bow_pipe),\n",
    "    #('simple_pipeline', simple_pipe),\n",
    "    #('syntax_pipeline', syntax_pipe)\n",
    "    #]),\n",
    "    # SelectKBest(chi2, k=100000),\n",
    "    # SelectFromModel(LinearSVC()),\n",
    "    #LogisticRegression())\n",
    "\n",
    "clf = Pipeline([('vect', FeatureUnion(transformer_list=[\n",
    "                ('length_pipeline', length_pipe),\n",
    "                ('bow_pipeline', bow_pipe),\n",
    "                ('simple_pipeline', simple_pipe),\n",
    "                #('syntax_pipeline', syntax_pipe)\n",
    "                ])), \n",
    "                #('reducer', SelectKBest(chi2, k=100000)),\n",
    "                ('clf', LogisticRegression())\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['author', 'has_spouse', 'has_spouse', 'NO_REL', 'author', 'has_spouse', 'author', 'author', 'worked_at', 'author']\n",
      "42338\n",
      "9754\n"
     ]
    }
   ],
   "source": [
    "# test_data, test_labels = load_data('data/test.json.txt', verbose=False)\n",
    "# print(dev_labels[:10])\n",
    "\n",
    "# all_train = get_context(train_data, embed_mode=False)\n",
    "# all_dev = get_context(dev_data, embed_mode=False)\n",
    "# all_test = get_context(test_data, embed_mode=False)\n",
    "\n",
    "# print(all_train[:1])\n",
    "# DATA ExractSimpleFeatures\n",
    "#train_simple_featurized = ExractSimpleFeatures(train_data, verbose=False)\n",
    "#dev_simple_featurized = ExractSimpleFeatures(dev_data, verbose=False)\n",
    "#test_simple_featurized = ExractSimpleFeatures(test_data, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NO_REL' 'author' 'capital' 'has_spouse' 'worked_at']\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "# MODEL\n",
    "\n",
    "# Transform labels to nimeric values\n",
    "# le = LabelEncoder()\n",
    "# train_labels_featurized = le.fit_transform(train_labels)\n",
    "# dev_labels_featurized = le.transform(dev_labels)\n",
    "\n",
    "# print(le.classes_)\n",
    "\n",
    "# Fit model one vs rest logistic regression    \n",
    "# clf = make_pipeline(DictVectorizer(), LogisticRegression())\n",
    "\n",
    "# if with CountVectorizer\n",
    "# bow_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "# TFiDF_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# clf = make_pipeline(bow_vectorizer, LogisticRegression())\n",
    "# clf = LogisticRegression()\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('feats', FeatureUnion([\n",
    "#         ('countvec', bow_vectorizer), # can pass in either a pipeline\n",
    "#         ('transformer', FeatureTransformer()) # or a transformer\n",
    "#     ])),\n",
    "#     ('clf', LogisticRegression())  # classifier\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TRAIN CLASSIFIER AND EVALUATE (CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_statistics_header():\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "        'relation', 'precision', 'recall', 'f-score', 'support'))\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "        '-' * 18, '-' * 9, '-' * 9, '-' * 9, '-' * 9))\n",
    "\n",
    "def print_statistics_row(rel, result):\n",
    "    print('{:20s} {:10.3f} {:10.3f} {:10.3f} {:10d}'.format(rel, *result))\n",
    "\n",
    "def print_statistics_footer(avg_result):\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "        '-' * 18, '-' * 9, '-' * 9, '-' * 9, '-' * 9))\n",
    "    print('{:20s} {:10.3f} {:10.3f} {:10.3f} {:10d}'.format('macro-average', *avg_result))\n",
    "\n",
    "def macro_average_results(results):\n",
    "    avg_result = [np.average([r[i] for r in results.values()]) for i in range(3)]\n",
    "    avg_result.append(np.sum([r[3] for r in results.values()]))\n",
    "    return avg_result\n",
    "\n",
    "def average_results(results):\n",
    "    avg_result = [np.average([r[i] for r in results]) for i in range(3)]\n",
    "    avg_result.append(np.sum([r[3] for r in results]))\n",
    "    return avg_result\n",
    "    \n",
    "def evaluateCV(classifier, label_encoder, X, y, verbose=True):\n",
    "    results = {}\n",
    "    for rel in le.classes_:\n",
    "        results[rel] = []\n",
    "    if verbose:\n",
    "        print_statistics_header()\n",
    "        kfold = StratifiedKFold(n_splits = 5, shuffle=True, random_state=0) \n",
    "        for train_index, test_index in kfold.split(X, y):\n",
    "            # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "            y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "            clf.fit(X_train, y_train)\n",
    "            pred_labels = classifier.predict(X_test)\n",
    "            stats = precision_recall_fscore_support(y_test, pred_labels, beta=0.5)\n",
    "            # print(stats)\n",
    "            for rel in label_encoder.classes_:\n",
    "                rel_id = label_encoder.transform([rel])[0]\n",
    "                # print(rel_id,rel)\n",
    "                stats_rel = [stat[rel_id] for stat in stats]\n",
    "                results[rel].append(stats_rel)\n",
    "        for rel in label_encoder.classes_:\n",
    "            results[rel] = average_results(results[rel])\n",
    "            if verbose:\n",
    "                print_statistics_row(rel, results[rel])\n",
    "    avg_result = macro_average_results(results)\n",
    "    if verbose:\n",
    "        print_statistics_footer(avg_result)\n",
    "    return avg_result[2]  # return f_0.5 score as summary statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support\n",
      "------------------    ---------  ---------  ---------  ---------\n",
      "NO_REL                    0.778      0.401      0.654       3068\n",
      "author                    0.932      0.971      0.939      13113\n",
      "capital                   0.940      0.973      0.946       9427\n",
      "has_spouse                0.910      0.977      0.923      13061\n",
      "worked_at                 0.905      0.816      0.886       3669\n",
      "------------------    ---------  ---------  ---------  ---------\n",
      "macro-average             0.893      0.828      0.870      42338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8696233397953861"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is without dependency info and it takes me around 10 minutes to train it.\n",
    "# But I did not change anything from your model I guess...\n",
    "evaluateCV(clf, le, train_data, train_labels_featurized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A check for the average F1 score\n",
    "\n",
    "f_scorer = make_scorer(fbeta_score, beta=0.5, average='macro')\n",
    "\n",
    "def evaluateCV_check(classifier, X, y, verbose=True):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle=True, random_state=0) \n",
    "    scores = cross_val_score(classifier, X, y, cv=kfold, scoring = f_scorer)\n",
    "    print(\"\\nCross-validation scores (StratifiedKFold): \", scores)\n",
    "    print(\"Mean cv score (StratifiedKFold): \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation scores (StratifiedKFold):  [0.77831464 0.77490296 0.78322361 0.77969274 0.78449379]\n",
      "Mean cv score (StratifiedKFold):  0.7801255472002809\n"
     ]
    }
   ],
   "source": [
    "evaluateCV_check(clf, all_train, train_labels_featurized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TEST PREDICTIONS and ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit final model on the full train data\n",
    "clf.fit(all_train, train_labels_featurized)\n",
    "\n",
    "# Predict on test set\n",
    "# dev_label_predicted = clf.predict(all_dev)\n",
    "test_label_predicted = clf.predict(all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  ['NO_REL', 'author', 'has_spouse', 'capital', 'worked_at']\n",
      "[1 3 3 0 1 3 1 1 4 1 4 0 1 1 0 0 1 2 2 2 3 4 1 0 0 3 3 3 3 0]\n",
      "[1 3 3 0 1 3 1 1 1 1 0 0 1 1 0 4 1 2 2 0 3 4 1 0 0 3 3 3 3 0]\n",
      "0.7859857572352148\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     NO_REL       0.66      0.73      0.70       473\n",
      "     author       0.84      0.82      0.83       529\n",
      " has_spouse       0.86      0.57      0.68       111\n",
      "    capital       0.86      0.89      0.87       593\n",
      "  worked_at       0.73      0.65      0.69       226\n",
      "\n",
      "avg / total       0.79      0.79      0.79      1932\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FOR DEV TESTING\n",
    "y_true = dev_labels_featurized\n",
    "labels = list(set(train_labels))\n",
    "\n",
    "final_score = f1_score(y_true, dev_label_predicted, average='weighted')\n",
    "print(\"Labels: \", labels)\n",
    "print(y_true[:30])\n",
    "print(dev_label_predicted[:30])\n",
    "\n",
    "print(final_score)\n",
    "print(classification_report(y_true, dev_label_predicted, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['capital' 'NO_REL' 'worked_at' 'NO_REL' 'has_spouse']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inigma/anaconda3/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# ON TEST DATA: TO UPLOAD\n",
    "# Deprecation warning explained: https://stackoverflow.com/questions/49545947/sklearn-deprecationwarning-truth-value-of-an-array\n",
    "test_label_predicted_decoded = le.inverse_transform(test_label_predicted)\n",
    "print(test_label_predicted_decoded[:5])\n",
    "f = open(\"test_labels.txt\", 'w', encoding=\"utf-8\")\n",
    "for label in test_label_predicted_decoded:\n",
    "    f.write(label+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features used to predict: \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'named_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-1bcdbd1eda6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top features used to predict: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# show the top features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprintNMostInformative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-146-1bcdbd1eda6d>\u001b[0m in \u001b[0;36mprintNMostInformative\u001b[0;34m(classifier, label_encoder, N)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprintNMostInformative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"Prints features with the highest coefficient values, per class\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dictvectorizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcoef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logisticregression'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'named_steps'"
     ]
    }
   ],
   "source": [
    "# Feature analisys - print N most informative\n",
    "# !! Make changes in this function when you change the pipleine!!\n",
    "def printNMostInformative(classifier, label_encoder, N):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = classifier.named_steps['dictvectorizer'].get_feature_names()\n",
    "\n",
    "    coef = classifier.named_steps['logisticregression'].coef_    \n",
    "    print(coef.shape)\n",
    "    for rel in label_encoder.classes_:\n",
    "        rel_id = label_encoder.transform([rel])[0]\n",
    "        coef_rel = coef[rel_id]\n",
    "        coefs_with_fns = sorted(zip(coef_rel, feature_names))\n",
    "        top_features = coefs_with_fns[-N:]\n",
    "        print(\"\\nClass {} best: \".format(rel))\n",
    "        for feat in top_features:\n",
    "            print(feat)        \n",
    "        \n",
    "print(\"Top features used to predict: \")\n",
    "# show the top features\n",
    "printNMostInformative(clf, le, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
