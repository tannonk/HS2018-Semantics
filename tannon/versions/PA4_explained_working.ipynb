{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, fbeta_score, make_scorer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import FunctionTransformer,LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Classification\n",
    "Our prediction problem in this task is given an input pair of entities to predict what relation the pair belongs to. This is a more simplified problem of the general task of relation extraction. Entities are represented by Wiki IDs (that is, suffixes of Wikipedia URLs). The set of possible relations is fixed. We assume that each pair can be only in one relation (This is multi-class classification.) We will build a classifier using supervised machine learning techniques (logistic regression).\n",
    "\n",
    "### Dataset\n",
    "Producing labeled data for the general task of relation extraction is expensive. An alternative approach is to use distant supervision (Mintz et al. 2009 Distant supervision for relation extraction without labeled data). The dataset we will use is build using this method. First, a corpus was derived from the expanded version of Wikilinks where each example includes a pair of entities and the context around the entity mention. (This expanded version was build on the Wikilinks dataset announced by Google in 2013. This dataset contains over 40M mentions of 3M distinct entities spanning 10M webpages. It provides entity resolutions by mapping each entity mention to a Wikipedia URL.) The entities are named in the corpus using Wiki IDs, which you can think of as the last portion of a Wikipedia URL. Thus the Wiki ID Barack_Obama designates the entity described by https://en.wikipedia.org/wiki/Barack_Obama. Next, in order to annotate each pair of entity mentions with a relation (if any), the corpus is connected with an external source of knowledge about relations, i.e. knowledge base. Our dataset is biult using a knowledge base (KB) ultimately derived from Freebase. (Unfortunately, Freebase was shut down in 2016, but the Freebase data is still available from various sources and in various forms, i.e. Freebase Easy data dump.) Ultimately, the KB provides the labels for a given entity pair in database, and the corpus provides the features, i.e. all the examples from the corpus where those two entities co-occur are used to generate a feature representation describing the entity pair. Such dataset is used to train relation classification system which can be used further to extract new relation in order to populate KB.\n",
    "\n",
    "Our dataset for this task 'train.json.txt' is in JSON format. The dataset includes examples for 4 relations: 'worked_at', 'capital', 'has_spouse' and 'author'. There is no overlap between relation in our dataset, i.e. each pair is in one relation which makes it possible to apply multi-class classification algorithms(as opposed to multi-label classification). In addition, there are instances which are not related ('NO_REL' label), however this data portion is noisy. They are derived using a pair of entities from the corpus which are not in KB. Since KB is not complete, some of these supposedly negative instances may be false negatives. A pair of entities might be related in real life even if they don't appear together in the KB.\n",
    "\n",
    "## Evaluation\n",
    "We will use precision and recall reported separately for each relation (label). To combine these two metrics into one for each class, we will use a weighted F0.5-score, which gives precision twice as much weight as recall. This is motivated by our problem: if we're (ulimately) extracting new relation triples from (massively abundant) text on the web in order to augment a knowledge base, it's probably more important that the triples we extract are correct (precision) than that we extract all the triples we could (recall). We aggregate our metrics across all relations using macro-averaging which gives equal weight to all relations, and thus give lesser weight to problem instances in relations with more instances (the number of instances per relation is, to some degree, an accident of the data collection methodology). Your system should be optimized for the macro-averaged F0.5-score calcualted on the dataset using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json example:\n",
      "{'relation': 'has_spouse', 'entity_1': 'Judy_Garland', 'entity_2': 'David_Rose', 'snippet': [{'left': 'thirty and his life and career were riding high . In 1941 , shortly after the death of his father , Mercer began an intense affair with nineteen-year-old', 'mention_1': 'Judy Garland', 'middle': 'while she was engaged to composer', 'mention_2': 'David Rose', 'right': '. Garland married Rose to temporarily stop the affair , but the effect on Mercer lingered , adding to the emotional depth of his lyrics . Their affair', 'direction': 'fwd'}]}\n",
      "\n",
      "example transformed as a named tuple:\n",
      "PairExample(entity_1='Judy_Garland', entity_2='David_Rose', snippet=[Snippet(left='thirty and his life and career were riding high . In 1941 , shortly after the death of his father , Mercer began an intense affair with nineteen-year-old', mention_1='Judy Garland', middle='while she was engaged to composer', mention_2='David Rose', right='. Garland married Rose to temporarily stop the affair , but the effect on Mercer lingered , adding to the emotional depth of his lyrics . Their affair', direction='fwd')])\n"
     ]
    }
   ],
   "source": [
    "############################################################################################\n",
    "# 1. LOAD DATA\n",
    "############################################################################################\n",
    "\n",
    "PairExample = namedtuple('PairExample',\n",
    "    'entity_1, entity_2, snippet')\n",
    "Snippet = namedtuple('Snippet',\n",
    "    'left, mention_1, middle, mention_2, right, direction')\n",
    "def load_data(file, verbose=True):\n",
    "    f = open(file,'r', encoding='utf-8')\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i,line in enumerate(f):\n",
    "        instance = json.loads(line)\n",
    "        if i==0:\n",
    "            if verbose:\n",
    "                print('json example:')\n",
    "                print(instance)\n",
    "        #'relation, entity_1, entity_2, snippet' fileds for each example\n",
    "        #'left, mention_1, middle, mention_2, right, direction' for each snippet\n",
    "        instance_tuple = PairExample(instance['entity_1'],instance['entity_2'],[])\n",
    "        for snippet in instance['snippet']:\n",
    "            try:\n",
    "                snippet_tuple = Snippet(snippet['left'],snippet['mention_1'],\n",
    "                                        snippet['middle'], \n",
    "                                        snippet['mention_2'],snippet['right'],\n",
    "                                        snippet['direction'])\n",
    "                instance_tuple.snippet.append(snippet_tuple)\n",
    "            except:\n",
    "                print(instance)\n",
    "        if i==0:\n",
    "            if verbose:\n",
    "                print('\\nexample transformed as a named tuple:')\n",
    "                print(instance_tuple)\n",
    "        data.append(instance_tuple)\n",
    "        labels.append(instance['relation'])\n",
    "    return data,labels\n",
    "    \n",
    "train_data, train_labels = load_data('train.json.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data[0])\n",
    "# print()\n",
    "# print(train_labels[0])\n",
    "\n",
    "# print(train_data[0].entity_1)\n",
    "# print(train_data[0].entity_2)\n",
    "# print(train_data[0].snippet)\n",
    "\n",
    "# print(train_data[0].snippet[0].left)\n",
    "# print(train_data[0].snippet[0].mention_1)\n",
    "# print(train_data[0].snippet[0].middle)\n",
    "# print(train_data[0].snippet[0].mention_2)\n",
    "# print(train_data[0].snippet[0].right)\n",
    "# print(train_data[0].snippet[0].direction)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set statistics:\n",
      "                                rel_examples\n",
      "relation               examples /all_examples\n",
      "--------               --------    -------\n",
      "has_spouse                 3019       0.31\n",
      "author                     2653       0.27\n",
      "NO_REL                     2300       0.24\n",
      "capital                     510       0.05\n",
      "worked_at                  1178       0.12\n",
      "--------               --------    -------\n",
      "Total                      9660       1.00\n"
     ]
    }
   ],
   "source": [
    "# Statistics over relations\n",
    "def print_stats(labels):\n",
    "    labels_counts = Counter(labels)\n",
    "    print('{:20s} {:>10s} {:>10s}'.format('', '', 'rel_examples'))\n",
    "    print('{:20s} {:>10s} {:>10s}'.format('relation', 'examples', '/all_examples'))\n",
    "    print('{:20s} {:>10s} {:>10s}'.format('--------', '--------', '-------'))\n",
    "    for k,v in labels_counts.items():\n",
    "        print('{:20s} {:10d} {:10.2f}'.format(k, v, v /len(labels)))\n",
    "    print('{:20s} {:>10s} {:>10s}'.format('--------', '--------', '-------'))\n",
    "    print('{:20s} {:10d} {:10.2f}'.format('Total', len(labels), len(labels) /len(labels)))\n",
    "\n",
    "print('Train set statistics:')\n",
    "print_stats(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done building dictionary\n",
      "\n",
      "has_spouse Judy_Garland David_Rose\n",
      "author Charlie_and_the_Chocolate_Factory Roald_Dahl\n",
      "NO_REL Sichuan Tibet\n",
      "capital Andalusia Seville\n",
      "worked_at Carl-Henric_Svanberg Ericsson\n"
     ]
    }
   ],
   "source": [
    "# check that each entity pair is assigned only one relation\n",
    "pair_dict={}\n",
    "rel_dict={}\n",
    "for example, label in zip(train_data,train_labels):\n",
    "    if (example.entity_1,example.entity_2) not in pair_dict.keys():\n",
    "        pair_dict[(example.entity_1,example.entity_2)] = [label]\n",
    "        \n",
    "    else:\n",
    "        pair_dict[(example.entity_1,example.entity_2)].append(label)\n",
    "        print(example.entity_1,example.entity_2,label)\n",
    "    if label not in rel_dict.keys():\n",
    "        rel_dict[label] = [example]\n",
    "    else:\n",
    "        rel_dict[label].append(example)\n",
    "print(\"Done building dictionary\\n\")  \n",
    "    \n",
    "# example for each relation\n",
    "for rel in rel_dict.keys():\n",
    "    ex = rel_dict[rel][0]\n",
    "    print(rel,ex.entity_1,ex.entity_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspect pair_dict\n",
    "# for k, v in pair_dict.items():\n",
    "#     print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to reconstruct full context\n",
    "# ex = train_data[0]\n",
    "# print(ex)\n",
    "# print(\"\\n full context:\")\n",
    "# s = ex.snippet[0]\n",
    "# print(' '.join((s.left, s.mention_1, s.middle, s.mention_2, s.right)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=aCdg-d_476Y\n",
    "# vectorizer = CountVectorizer()\n",
    "# BOW = vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### EDIT FOR BOW MODEL\n",
    "\n",
    "# ###########################################################################################\n",
    "# # 2. EXTRACT FEATURES and BUILD CLASSIFIER\n",
    "# ###########################################################################################\n",
    "\n",
    "# # Extract two simple features\n",
    "# def ExractSimpleFeatures(data, verbose=True):\n",
    "#     featurized_data = []\n",
    "#     for instance in data:\n",
    "#         featurized_instance = {'left_context': '', 'right_context': '', 'mid_words':''}\n",
    "#         for s in instance.snippet:\n",
    "# #             if len(s.middle.split()) < featurized_instance['distance']:\n",
    "#             featurized_instance['mid_words'] = vectorizer.transform(s.middle.split())\n",
    "# #                 featurized_instance['distance'] = len(s.middle.split())\n",
    "#             featurized_instance['left_context'] = vectorizer.transform(s.left.split())\n",
    "#             featurized_instance['right_context'] = vectorizer.transform(s.right.split())\n",
    "#         featurized_data.append(featurized_instance)\n",
    "#     if verbose:\n",
    "#         print(len(data))\n",
    "#         print(len(featurized_data))\n",
    "#         print(data[0])\n",
    "#         print(featurized_data[0])\n",
    "#         print(featurized_data[1])\n",
    "#     return featurized_data\n",
    "\n",
    "# # Transform dataset to features\n",
    "# train_data_featurized = ExractSimpleFeatures(train_data)\n",
    "\n",
    "# # Transform labels to nimeric values\n",
    "# le = LabelEncoder()\n",
    "# train_labels_featurized = le.fit_transform(train_labels)\n",
    "\n",
    "# # Fit model one vs rest logistic regression    \n",
    "# clf = make_pipeline(DictVectorizer(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in train_data_featurized[0].items():\n",
    "#     print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9660\n",
      "9660\n",
      "PairExample(entity_1='Judy_Garland', entity_2='David_Rose', snippet=[Snippet(left='thirty and his life and career were riding high . In 1941 , shortly after the death of his father , Mercer began an intense affair with nineteen-year-old', mention_1='Judy Garland', middle='while she was engaged to composer', mention_2='David Rose', right='. Garland married Rose to temporarily stop the affair , but the effect on Mercer lingered , adding to the emotional depth of his lyrics . Their affair', direction='fwd')])\n",
      "{'left_context': 'thirty and his life and career were riding high . In 1941 , shortly after the death of his father , Mercer began an intense affair with nineteen-year-old', 'right_context': '. Garland married Rose to temporarily stop the affair , but the effect on Mercer lingered , adding to the emotional depth of his lyrics . Their affair', 'mid_words': 'while she was engaged to composer', 'distance': 6}\n",
      "{'left_context': 'in touch . Follow Letters of Note ... RSS | Email | Twitter Tumblr | Facebook Monday , 12 October 2009 Unrest at the Chocolate Factory 1964 :', 'right_context': 'is released , its Oompa Loompa characters depicted as a 3,000 strong tribe of black pygmies ( Pic ) , imported by Willy Wonka from the African', 'mid_words': 'by', 'distance': 1}\n"
     ]
    }
   ],
   "source": [
    "###########################################################################################\n",
    "# 2. EXTRACT FEATURES and BUILD CLASSIFIER\n",
    "###########################################################################################\n",
    "\n",
    "# Extract two simple features\n",
    "def ExractSimpleFeatures(data, verbose=True):\n",
    "    featurized_data = []\n",
    "    for instance in data:\n",
    "        featurized_instance = {'left_context': '', 'right_context': '', 'mid_words':'', 'distance':np.inf}\n",
    "        for s in instance.snippet:\n",
    "            if len(s.middle.split()) < featurized_instance['distance']:\n",
    "                featurized_instance['mid_words'] = s.middle\n",
    "                featurized_instance['distance'] = len(s.middle.split())\n",
    "            featurized_instance['left_context'] = s.left\n",
    "            featurized_instance['right_context'] = s.right\n",
    "        featurized_data.append(featurized_instance)\n",
    "    if verbose:\n",
    "        print(len(data))\n",
    "        print(len(featurized_data))\n",
    "        print(data[0])\n",
    "        print(featurized_data[0])\n",
    "        print(featurized_data[1])\n",
    "    return featurized_data\n",
    "\n",
    "# Transform dataset to features\n",
    "train_data_featurized = ExractSimpleFeatures(train_data)\n",
    "\n",
    "# Transform labels to nimeric values\n",
    "le = LabelEncoder()\n",
    "train_labels_featurized = le.fit_transform(train_labels)\n",
    "\n",
    "# Fit model one vs rest logistic regression    \n",
    "clf = make_pipeline(DictVectorizer(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# 3. TRAIN CLASSIFIER AND EVALUATE (CV)\n",
    "##################################################################################################\n",
    "\n",
    "def print_statistics_header():\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "        'relation', 'precision', 'recall', 'f-score', 'support'))\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "        '-' * 18, '-' * 9, '-' * 9, '-' * 9, '-' * 9))\n",
    "\n",
    "def print_statistics_row(rel, result):\n",
    "    print('{:20s} {:10.3f} {:10.3f} {:10.3f} {:10d}'.format(rel, *result))\n",
    "\n",
    "def print_statistics_footer(avg_result):\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "        '-' * 18, '-' * 9, '-' * 9, '-' * 9, '-' * 9))\n",
    "    print('{:20s} {:10.3f} {:10.3f} {:10.3f} {:10d}'.format('macro-average', *avg_result))\n",
    "\n",
    "def macro_average_results(results):\n",
    "    avg_result = [np.average([r[i] for r in results.values()]) for i in range(3)]\n",
    "    avg_result.append(np.sum([r[3] for r in results.values()]))\n",
    "    return avg_result\n",
    "\n",
    "def average_results(results):\n",
    "    avg_result = [np.average([r[i] for r in results]) for i in range(3)]\n",
    "    avg_result.append(np.sum([r[3] for r in results]))\n",
    "    return avg_result\n",
    "    \n",
    "def evaluateCV(classifier, label_encoder, X, y, verbose=True):\n",
    "    results = {}\n",
    "    for rel in le.classes_:\n",
    "            results[rel] = []\n",
    "    if verbose:\n",
    "        print_statistics_header()\n",
    "        kfold = StratifiedKFold(n_splits = 5, shuffle=True, random_state=0) \n",
    "        for train_index, test_index in kfold.split(X, y):\n",
    "            #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "            y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "            clf.fit(X_train, y_train)\n",
    "            pred_labels = classifier.predict(X_test)\n",
    "            stats = precision_recall_fscore_support(y_test, pred_labels, beta=0.5)\n",
    "            #print(stats)\n",
    "            for rel in label_encoder.classes_:\n",
    "                rel_id = label_encoder.transform([rel])[0]\n",
    "            #print(rel_id,rel)\n",
    "                stats_rel = [stat[rel_id] for stat in stats]\n",
    "                results[rel].append(stats_rel)\n",
    "        for rel in label_encoder.classes_:\n",
    "            results[rel] = average_results(results[rel])\n",
    "            if verbose:\n",
    "                print_statistics_row(rel, results[rel])\n",
    "    avg_result = macro_average_results(results)\n",
    "    if verbose:\n",
    "        print_statistics_footer(avg_result)\n",
    "    return avg_result[2]  # return f_0.5 score as summary statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support\n",
      "------------------    ---------  ---------  ---------  ---------\n",
      "NO_REL                    0.430      0.716      0.467       2300\n",
      "author                    0.668      0.687      0.672       2653\n",
      "capital                   0.630      0.067      0.212        510\n",
      "has_spouse                0.886      0.782      0.863       3019\n",
      "worked_at                 0.757      0.238      0.523       1178\n",
      "------------------    ---------  ---------  ---------  ---------\n",
      "macro-average             0.674      0.498      0.547       9660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5473472778168718"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateCV(clf,le,train_data_featurized,train_labels_featurized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A check for the average F1 score\n",
    "\n",
    "f_scorer = make_scorer(fbeta_score, beta=0.5, average='macro')\n",
    "\n",
    "def evaluateCV_check(classifier, X, y, verbose=True):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle=True, random_state=0) \n",
    "    scores = cross_val_score(classifier, X, y, cv=kfold, scoring = f_scorer)\n",
    "    print(\"\\nCross-validation scores (StratifiedKFold): \", scores)\n",
    "    print(\"Mean cv score (StratifiedKFold): \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation scores (StratifiedKFold):  [0.55181076 0.57501018 0.55481108 0.49823873 0.55686564]\n",
      "Mean cv score (StratifiedKFold):  0.5473472778168718\n"
     ]
    }
   ],
   "source": [
    "evaluateCV_check(clf,train_data_featurized,train_labels_featurized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NO_REL' 'NO_REL' 'author' 'author' 'has_spouse' 'author' 'author'\n",
      " 'has_spouse' 'author' 'NO_REL']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tannon/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#########################################################################################\n",
    "# 4. TEST PREDICTIONS and ANALYSIS\n",
    "#########################################################################################\n",
    "\n",
    "# Fit final model on the full train data\n",
    "clf.fit(train_data_featurized, train_labels_featurized)\n",
    "\n",
    "# Predict on test set\n",
    "test_data, test_labels = load_data('test-covered.json.txt', verbose=False)\n",
    "test_data_featurized = ExractSimpleFeatures(test_data, verbose=False)\n",
    "test_label_predicted = clf.predict(test_data_featurized)\n",
    "# Deprecation warning explained: https://stackoverflow.com/questions/49545947/sklearn-deprecationwarning-truth-value-of-an-array\n",
    "test_label_predicted_decoded = le.inverse_transform(test_label_predicted)\n",
    "print(test_label_predicted_decoded[:10])\n",
    "f = open(\"test_labels.txt\", 'w', encoding=\"utf-8\")\n",
    "for label in test_label_predicted_decoded:\n",
    "    f.write(label+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features used to predict: \n",
      "(5, 21612)\n",
      "\n",
      "Class NO_REL best: \n",
      "(1.4250601622301795, 'mid_words=or')\n",
      "(1.5156828233132422, 'mid_words=and the')\n",
      "(1.9545665885919097, 'mid_words=, and')\n",
      "\n",
      "Class author best: \n",
      "(3.1310845722002716, \"mid_words='s novel\")\n",
      "(3.2968166300549115, 'mid_words=, by')\n",
      "(4.9548165954293335, 'mid_words=by')\n",
      "\n",
      "Class capital best: \n",
      "(1.9743402861695774, 'mid_words=, in')\n",
      "(2.8272103467885317, 'mid_words=in')\n",
      "(2.9943715995150084, 'mid_words=after')\n",
      "\n",
      "Class has_spouse best: \n",
      "(3.284436384356054, 'mid_words=married')\n",
      "(3.339915685104778, 'mid_words=&')\n",
      "(4.044216459098745, 'mid_words=and his wife')\n",
      "\n",
      "Class worked_at best: \n",
      "(2.4611260642169825, 'mid_words=professor')\n",
      "(2.6143582378202037, 'mid_words=CEO')\n",
      "(2.7715834402382065, 'mid_words=of the')\n"
     ]
    }
   ],
   "source": [
    "# Feature analisys - print N most informative\n",
    "# !! Make changes in this function when you change the pipleine!!\n",
    "def printNMostInformative(classifier,label_encoder,N):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = classifier.named_steps['dictvectorizer'].get_feature_names()\n",
    "\n",
    "    coef = classifier.named_steps['logisticregression'].coef_    \n",
    "    print(coef.shape)\n",
    "    for rel in label_encoder.classes_:\n",
    "        rel_id = label_encoder.transform([rel])[0]\n",
    "        coef_rel = coef[rel_id]\n",
    "        coefs_with_fns = sorted(zip(coef_rel, feature_names))\n",
    "        top_features = coefs_with_fns[-N:]\n",
    "        print(\"\\nClass {} best: \".format(rel))\n",
    "        for feat in top_features:\n",
    "            print(feat)        \n",
    "        \n",
    "print(\"Top features used to predict: \")\n",
    "# show the top features\n",
    "printNMostInformative(clf,le,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
