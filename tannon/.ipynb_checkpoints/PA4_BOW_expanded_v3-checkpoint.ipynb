{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this script sets a baseline for realation extraction using frequency-based BOW model\n",
    "\n",
    "#### add additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, fbeta_score, make_scorer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import FunctionTransformer,LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "##### additional imports\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json example:\n",
      "{'relation': 'has_spouse', 'entity_1': 'Judy_Garland', 'entity_2': 'David_Rose', 'snippet': [{'left': 'thirty and his life and career were riding high . In 1941 , shortly after the death of his father , Mercer began an intense affair with nineteen-year-old', 'mention_1': 'Judy Garland', 'middle': 'while she was engaged to composer', 'mention_2': 'David Rose', 'right': '. Garland married Rose to temporarily stop the affair , but the effect on Mercer lingered , adding to the emotional depth of his lyrics . Their affair', 'direction': 'fwd'}]}\n",
      "\n",
      "example transformed as a named tuple:\n",
      "PairExample(entity_1='Judy_Garland', entity_2='David_Rose', snippet=[Snippet(left='thirty and his life and career were riding high . In 1941 , shortly after the death of his father , Mercer began an intense affair with nineteen-year-old', mention_1='Judy Garland', middle='while she was engaged to composer', mention_2='David Rose', right='. Garland married Rose to temporarily stop the affair , but the effect on Mercer lingered , adding to the emotional depth of his lyrics . Their affair', direction='fwd')])\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################\n",
    "# 1. LOAD DATA\n",
    "##################################################################################################\n",
    "\n",
    "PairExample = namedtuple('PairExample',\n",
    "    'entity_1, entity_2, snippet')\n",
    "Snippet = namedtuple('Snippet',\n",
    "    'left, mention_1, middle, mention_2, right, direction')\n",
    "def load_data(file, verbose=True):\n",
    "    f = open(file,'r', encoding='utf-8')\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i,line in enumerate(f):\n",
    "        instance = json.loads(line)\n",
    "        if i==0:\n",
    "            if verbose:\n",
    "                print('json example:')\n",
    "                print(instance)\n",
    "        #'relation, entity_1, entity_2, snippet' fileds for each example\n",
    "        #'left, mention_1, middle, mention_2, right, direction' for each snippet\n",
    "        instance_tuple = PairExample(instance['entity_1'],instance['entity_2'],[])\n",
    "        for snippet in instance['snippet']:\n",
    "            try:\n",
    "                snippet_tuple = Snippet(snippet['left'],snippet['mention_1'],snippet['middle'],\n",
    "                                   snippet['mention_2'],snippet['right'],\n",
    "                                    snippet['direction'])\n",
    "                instance_tuple.snippet.append(snippet_tuple)\n",
    "            except:\n",
    "                print(instance)\n",
    "        if i==0:\n",
    "            if verbose:\n",
    "                print('\\nexample transformed as a named tuple:')\n",
    "                print(instance_tuple)\n",
    "        data.append(instance_tuple)\n",
    "        labels.append(instance['relation'])\n",
    "    return data,labels\n",
    "    \n",
    "train_data, train_labels = load_data('../data/train.json.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set statistics:\n",
      "                                rel_examples\n",
      "relation               examples /all_examples\n",
      "--------               --------    -------\n",
      "has_spouse                 3019       0.31\n",
      "author                     2653       0.27\n",
      "NO_REL                     2300       0.24\n",
      "capital                     510       0.05\n",
      "worked_at                  1178       0.12\n",
      "--------               --------    -------\n",
      "Total                      9660       1.00\n"
     ]
    }
   ],
   "source": [
    "# Statistics over relations\n",
    "def print_stats(labels):\n",
    "    labels_counts = Counter(labels)\n",
    "    print('{:20s} {:>10s} {:>10s}'.format('', '', 'rel_examples'))\n",
    "    print('{:20s} {:>10s} {:>10s}'.format('relation', 'examples', '/all_examples'))\n",
    "    print('{:20s} {:>10s} {:>10s}'.format('--------', '--------', '-------'))\n",
    "    for k,v in labels_counts.items():\n",
    "        print('{:20s} {:10d} {:10.2f}'.format(k, v, v /len(labels)))\n",
    "    print('{:20s} {:>10s} {:>10s}'.format('--------', '--------', '-------'))\n",
    "    print('{:20s} {:10d} {:10.2f}'.format('Total', len(labels), len(labels) /len(labels)))\n",
    "\n",
    "print('Train set statistics:')\n",
    "print_stats(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done building dictionary\n",
      "has_spouse Judy_Garland David_Rose\n",
      "author Charlie_and_the_Chocolate_Factory Roald_Dahl\n",
      "NO_REL Sichuan Tibet\n",
      "capital Andalusia Seville\n",
      "worked_at Carl-Henric_Svanberg Ericsson\n"
     ]
    }
   ],
   "source": [
    "# check that each entity pair is assigned only one relation\n",
    "pair_dict={}\n",
    "rel_dict={}\n",
    "for example, label in zip(train_data,train_labels):\n",
    "    if (example.entity_1,example.entity_2) not in pair_dict.keys():\n",
    "        pair_dict[(example.entity_1,example.entity_2)] = [label]\n",
    "        \n",
    "    else:\n",
    "        pair_dict[(example.entity_1,example.entity_2)].append(label)\n",
    "        print(example.entity_1,example.entity_2,label)\n",
    "    if label not in rel_dict.keys():\n",
    "        rel_dict[label] = [example]\n",
    "    else:\n",
    "        rel_dict[label].append(example)\n",
    "print(\"Done building dictionary\")  \n",
    "    \n",
    "# example for each relation\n",
    "for rel in rel_dict.keys():\n",
    "    ex = rel_dict[rel][0]\n",
    "    print(rel,ex.entity_1,ex.entity_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to reconstruct full context\n",
    "\n",
    "# ex = train_data[0]\n",
    "# print(ex)\n",
    "# print(\"\\n full context:\")\n",
    "# s = ex.snippet[0]\n",
    "# print(' '.join((s.left, s.mention_1, s.middle, s.mention_2, s.right)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rebuild_text(ex):\n",
    "#     rebuilt_ex = []\n",
    "#     for s in ex.snippet:\n",
    "#         text = ' '.join((s.left, s.mention_1, s.middle, s.mention_2, s.right))\n",
    "#         rebuilt_ex.append(text)\n",
    "#     return rebuilt_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_text_from_snippet(s):\n",
    "#     text = ' '.join((s.left, s.mention_1, s.middle, s.mention_2, s.right))\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rebuild_corpus(data):\n",
    "#     corpus = []\n",
    "#     for ex in data:\n",
    "#         corpus.append(rebuild_text(ex)) \n",
    "#     return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_key_sents(data):\n",
    "#     key_sents = []\n",
    "#     for ex in data:\n",
    "#         m1 = ex.snippet[0].mention_1\n",
    "#         m2 = ex.snippet[0].mention_2\n",
    "#         text = build_text_from_snippet(ex.snippet[0])\n",
    "#         doc = nlp(text)\n",
    "#         for sent in doc.sents:\n",
    "# #             print(sent)\n",
    "#             if m1 in sent.string and m2 in sent.string:\n",
    "#                 key_sents.append(sent)\n",
    "#                 continue\n",
    "                \n",
    "#     return key_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_sents = extract_key_sents(train_data[:100])\n",
    "# print(type(key_sents[0]))\n",
    "# for sent in key_sents:\n",
    "#     for chunk in sent.noun_chunks:\n",
    "#         print(chunk.label_, chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "#           chunk.root.head.text)\n",
    "#     for token in sent: \n",
    "#         print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "#               [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_tokens(doc):\n",
    "    tagged_ex = []\n",
    "    \n",
    "    for w in doc:\n",
    "        if w.orth_ == \"MENTION_1\" or w.orth_ == \"MENTION_2\":\n",
    "            tagged_ex.append(w.orth_)\n",
    "        else:\n",
    "            tagged_ex.append(w.pos_)\n",
    "            \n",
    "    tagged_ex = \" \".join(tagged_ex)\n",
    "    \n",
    "    return tagged_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(doc):\n",
    "    lemmas = []\n",
    "    \n",
    "    for w in doc:\n",
    "        if w.lemma_ == \"-PRON-\" or w.orth_ == \"MENTION_1\" or w.orth_ == \"MENTION_2\":\n",
    "            lemmas.append(w.orth_)\n",
    "        else:\n",
    "            lemmas.append(w.lemma_)\n",
    "    \n",
    "    lemmas = \" \".join(lemmas)\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# 2.1 PERFORM NLP ON CORPUS DATA\n",
    "##################################################################################################\n",
    "\n",
    "def perform_nlp(data, verbose=True):\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"{} instances in data\".format(len(data)))\n",
    "        print(\"first instance looks like {}\".format(data[0]))\n",
    "        \n",
    "    c = 0\n",
    "    docs = []\n",
    "    for instance in data:\n",
    "        instance_context = []\n",
    "        for s in instance.snippet:\n",
    "            context = nlp(s.left + \" MENTION_1 \" + s.middle + \" MENTION_2 \" + s.right)\n",
    "            instance_context.append(context)\n",
    "        docs.append(instance_context)\n",
    "        c += 1\n",
    "    \n",
    "        if verbose:\n",
    "            if c % 1000 == 0:\n",
    "                print(\"{} instances processed.\".format(c))\n",
    "        \n",
    "    if verbose:\n",
    "        print(len(docs))\n",
    "        print(docs[0])\n",
    "        print(\"Structure of context data is: {}-{}-{}\".format(type(docs),\n",
    "                                                              type(docs[0]),\n",
    "                                                              type(docs[0][0])\n",
    "                                                             )\n",
    "             )\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# 2. EXTRACT FEATURES and BUILD CLASSIFIER\n",
    "##################################################################################################\n",
    "\n",
    "# Turn data into numerical features\n",
    "\n",
    "def SelectContext(data, verbose=True):\n",
    "    only_context_data = []\n",
    "    for instance in data:\n",
    "        instance_context = []\n",
    "        for s in instance.snippet:\n",
    "            context = s.left + \" MENTION_1 \" + s.middle + \" MENTION_2 \" + s.right\n",
    "            instance_context.append(context)\n",
    "        only_context_data.append(' '.join(instance_context))\n",
    "    if verbose:\n",
    "        print(len(data))\n",
    "        print(len(only_context_data))\n",
    "        print(data[0])\n",
    "        print(only_context_data[0])\n",
    "    return only_context_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelectLemmatizedContext(processed_data, verbose=True):\n",
    "    lemmatized_data = []\n",
    "    for processed_instance in processed_data:\n",
    "        instance_lemmas = []\n",
    "        for doc in processed_instance:  \n",
    "            lemmas = lemmatize(doc)\n",
    "            instance_lemmas.append(lemmas)\n",
    "        lemmatized_data.append(' '.join(instance_lemmas))\n",
    "    if verbose:\n",
    "        print(len(processed_data))\n",
    "        print(len(lemmatized_data))\n",
    "        print(processed_data[0])\n",
    "        print(lemmatized_data[0])\n",
    "    return lemmatized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelectTaggedContext(processed_data, verbose=True):\n",
    "    tagged_data = []\n",
    "    for processed_instance in processed_data:\n",
    "        instance_tags = []\n",
    "        for doc in processed_instance:  \n",
    "            tags = tag_tokens(doc)\n",
    "            instance_tags.append(tags)\n",
    "        tagged_data.append(' '.join(instance_tags))\n",
    "    if verbose:\n",
    "        print(len(processed_data))\n",
    "        print(len(tagged_data))\n",
    "        print(processed_data[0])\n",
    "        print(tagged_data[0])\n",
    "    return tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect feature extractions\n",
    "# context_simple = SelectContext(train_data)\n",
    "# context_lemmas = SelectLemmatizedContext(processed_train_data)\n",
    "# context_tagged = SelectTaggedContext(processed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transform labels to numeric values\n",
    "le = LabelEncoder()\n",
    "train_labels_featurized = le.fit_transform(train_labels)\n",
    "\n",
    "print(train_labels_featurized.shape)\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('cv', CountVectorizer(ngram_range=(2, 3))), # creates normal bow model\n",
    "    ('tfidf', TfidfTransformer()), # passes bow model and transforms to tfidf\n",
    "    ('logit', LogisticRegression()), # passes transformer to LR\n",
    "])\n",
    "\n",
    "# clf.fit(train_data_featurized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe2 = Pipeline([\n",
    "#     ('u1', FeatureUnion([\n",
    "#         ('tfdif_features', Pipeline([\n",
    "#             ('cv', CountVectorizer()),\n",
    "#             ('tfidf', TfidfTransformer()),\n",
    "#         ])),\n",
    "#         ('pos_features', Pipeline([\n",
    "#             ('pos', PosTagMatrix(tokenizer=nltk.word_tokenize) ),\n",
    "#         ])),\n",
    "#     ])),\n",
    "#     ('logit', LogisticRegression()),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# 3. TRAIN CLASSIFIER AND EVALUATE (CV)\n",
    "##################################################################################################\n",
    "\n",
    "def print_statistics_header():\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "        'relation', 'precision', 'recall', 'f-score', 'support'))\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "        '-' * 18, '-' * 9, '-' * 9, '-' * 9, '-' * 9))\n",
    "\n",
    "def print_statistics_row(rel, result):\n",
    "    print('{:20s} {:10.3f} {:10.3f} {:10.3f} {:10d}'.format(rel, *result))\n",
    "\n",
    "def print_statistics_footer(avg_result):\n",
    "    print('{:20s} {:>10s} {:>10s} {:>10s} {:>10s}'.format(\n",
    "        '-' * 18, '-' * 9, '-' * 9, '-' * 9, '-' * 9))\n",
    "    print('{:20s} {:10.3f} {:10.3f} {:10.3f} {:10d}'.format('macro-average', *avg_result))\n",
    "\n",
    "def macro_average_results(results):\n",
    "    avg_result = [np.average([r[i] for r in results.values()]) for i in range(3)]\n",
    "    avg_result.append(np.sum([r[3] for r in results.values()]))\n",
    "    return avg_result\n",
    "\n",
    "def average_results(results):\n",
    "    avg_result = [np.average([r[i] for r in results]) for i in range(3)]\n",
    "    avg_result.append(np.sum([r[3] for r in results]))\n",
    "    return avg_result\n",
    "    \n",
    "def evaluateCV(classifier, label_encoder, X, y, verbose=True):\n",
    "    \"\"\"\n",
    "    classifier: clf - pipeline with CountVevtorizer and Logistic regression\n",
    "    label_encoder: le - label encoder\n",
    "    X: train data featurized\n",
    "    y: train labels featurized\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for rel in le.classes_:\n",
    "#         print(rel)\n",
    "        results[rel] = []\n",
    "    if verbose:\n",
    "        print_statistics_header()\n",
    "        kfold = StratifiedKFold(n_splits = 5, shuffle=True, random_state=0) \n",
    "        for train_index, test_index in kfold.split(X, y):\n",
    "            #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "            y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "            clf.fit(X_train, y_train)\n",
    "            pred_labels = classifier.predict(X_test)\n",
    "            stats = precision_recall_fscore_support(y_test, pred_labels, beta=0.5)\n",
    "            #print(stats)\n",
    "            for rel in label_encoder.classes_:\n",
    "                rel_id = label_encoder.transform([rel])[0]\n",
    "#             print(rel_id,rel)\n",
    "                stats_rel = [stat[rel_id] for stat in stats]\n",
    "                results[rel].append(stats_rel)\n",
    "        for rel in label_encoder.classes_:\n",
    "            results[rel] = average_results(results[rel])\n",
    "            if verbose:\n",
    "                print_statistics_row(rel, results[rel])\n",
    "    avg_result = macro_average_results(results)\n",
    "    if verbose:\n",
    "        print_statistics_footer(avg_result)\n",
    "    return avg_result[2]  # return f_0.5 score as summary statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateCV(clf, le, train_data_featurized, train_labels_featurized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A check for the average F1 score\n",
    "\n",
    "f_scorer = make_scorer(fbeta_score, beta=0.5, average='macro')\n",
    "\n",
    "def evaluateCV_check(classifier, X, y, verbose=True):\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle=True, random_state=0) \n",
    "    scores = cross_val_score(classifier, X, y, cv=kfold, scoring = f_scorer)\n",
    "    print(\"\\nCross-validation scores (StratifiedKFold): \", scores)\n",
    "    print(\"Mean cv score (StratifiedKFold): \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateCV_check(clf,train_data_featurized,train_labels_featurized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# 4. TEST PREDICTIONS and ANALYSIS\n",
    "##################################################################################################\n",
    "\n",
    "# Fit final model on the full train data\n",
    "clf.fit(train_data_featurized, train_labels_featurized)\n",
    "\n",
    "# Predict on test set\n",
    "test_data, test_labels = load_data('../data/test-covered.json.txt', verbose=False)\n",
    "print(len(test_labels))\n",
    "test_data_featurized = SelectContext(test_data, verbose=False)\n",
    "test_label_predicted = clf.predict(test_data_featurized)\n",
    "print(len(test_label_predicted))\n",
    "# Deprecation warning explained: https://stackoverflow.com/questions/49545947/sklearn-deprecationwarning-truth-value-of-an-array\n",
    "test_label_predicted_decoded = le.inverse_transform(test_label_predicted)\n",
    "print(len(test_label_predicted_decoded))\n",
    "print(test_label_predicted_decoded[:2])\n",
    "f = open(\"outputs/test_labels.txt\", 'w', encoding=\"utf-8\")\n",
    "for label in test_label_predicted_decoded:\n",
    "    f.write(label+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature analisys - print N most informative\n",
    "# !! Make changes in this function when you change the pipleine!!\n",
    "def printNMostInformative(classifier,label_encoder,N):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = classifier.named_steps['countvectorizer'].get_feature_names()\n",
    "\n",
    "    coef = classifier.named_steps['logisticregression'].coef_    \n",
    "    print(coef.shape)\n",
    "    for rel in label_encoder.classes_:\n",
    "        rel_id = label_encoder.transform([rel])[0]\n",
    "        coef_rel = coef[rel_id]\n",
    "        coefs_with_fns = sorted(zip(coef_rel, feature_names))\n",
    "        top_features = coefs_with_fns[-N:]\n",
    "        print(\"\\nClass {} best: \".format(rel))\n",
    "        for feat in top_features:\n",
    "            print(feat)        \n",
    "        \n",
    "print(\"Top features used to predict: \")\n",
    "# show the top features\n",
    "printNMostInformative(clf,le,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
